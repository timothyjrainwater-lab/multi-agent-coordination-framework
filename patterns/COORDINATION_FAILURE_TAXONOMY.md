# Pattern: Coordination Failure Taxonomy

## Problem

Multi-agent LLM projects fail in predictable ways. But because each failure happens in a different context window, nobody accumulates enough experience to see the patterns. The same mistakes repeat across sessions, across projects, and across teams — because the failure modes aren't cataloged.

## Solution

Maintain a categorized failure catalog. Every coordination failure gets documented with its root cause, how it was detected, and what prevents recurrence. Over time, this becomes the most valuable artifact in the methodology — it's the accumulated institutional knowledge that no single agent can hold.

## The Taxonomy

Six categories of coordination failure, discovered empirically:

### Category 1: Partial Update
**What:** One session updates some files that reference a shared fact, but misses others.
**Why:** No consistency gate. The agent updated what it remembered, not what exists.
**Example:** Verification session reclassified a bug in the checklist and master list, but missed the domain-specific file. Next session saw conflicting numbers.
**Prevention:** Cross-file consistency gate — when any status/count/verdict changes, update ALL files that reference it in the same commit.

### Category 2: Cross-Provider Contamination
**What:** Work product from one LLM provider conflicts with in-flight work from another.
**Why:** Different providers have different assumptions, naming conventions, and confidence patterns. Work orders generated by one LLM may not be compatible with another's execution style.
**Example:** GPT-generated work orders conflicted with Claude-executed fix work, producing scope overlaps and contradictory instructions.
**Prevention:** All work orders go through a single dispatch authority (human or designated PM). No agent generates work orders for other agents without review.

### Category 3: Scope Bleed
**What:** A session modifies files outside its declared scope, conflicting with another concurrent session.
**Why:** Agent sees an "easy fix" in a nearby file and makes it, not knowing another session owns that file.
**Example:** Governance session auto-executed code changes on context rollover, potentially conflicting with a parallel fix session.
**Prevention:** Explicit session scope declaration. Agents must NOT write to files outside their declared scope without operator approval.

### Category 4: Context Starvation
**What:** A session lacks documents it needs because the dispatch didn't reference them.
**Why:** Dispatch author assumed the agent would "know" to check certain files, but the agent has zero prior context.
**Example:** Verification agents didn't cross-reference research documents because the dispatches didn't mention them. Result: 8-10 bugs flagged incorrectly, requiring reclassification.
**Prevention:** Dispatch self-containment — every dispatch must reference all documents the agent needs. If a document is relevant, it must be in the dispatch's "Files to Read" section.

### Category 5: Parallel Collision
**What:** Two concurrent sessions modify the same file, creating merge conflicts or silent overwrites.
**Why:** No file-level ownership protocol. Both sessions assumed they had exclusive access.
**Example:** Two fix work orders targeting functions in the same file, dispatched to parallel sessions.
**Prevention:** File ownership rules — if two work orders touch the same file, they must run sequentially, not in parallel. Work order dispatch must check for file-level conflicts.

### Category 6: Stale Reference
**What:** A document references another document that has moved, been renamed, or contains outdated numbers.
**Why:** Documents reference each other by path, but paths change when files are reorganized. Numbers (test counts, formula counts) change when work is completed.
**Example:** Onboarding checklist referenced a planning file at `docs/planning/FILE.md`, but the file had been moved to `docs/planning/archived/FILE.md`.
**Prevention:** Sources of truth index — a single file declaring which file is authoritative for each concept. Machine-generated sources (scripts that count tests, snapshot tools) outrank hand-maintained prose.

## Implementation

### Catalog Format

```markdown
### CF-[NNN]: [Short Name]

**Date:** [When it happened]
**Category:** [Partial Update | Cross-Provider | Scope Bleed | Context Starvation | Parallel Collision | Stale Reference]
**Sessions Involved:** [Which agents/sessions]
**What Happened:** [Factual description — no blame, just events]
**Root Cause:** [Why the coordination protocol didn't prevent it]
**Detection:** [How was it discovered]
**Resolution:** [What was done to fix it]
**Prevention:** [What governance change prevents recurrence]
**Status:** [MITIGATED | OPEN | ACCEPTED]
```

### When to Add Entries

- Any time you discover a conflict between sessions
- Any time an agent produces work that contradicts another agent's work
- Any time a human has to intervene because agents collided
- Any time a document is discovered to be wrong because of a coordination gap

### Reviewing the Catalog

At the start of each project phase or after a batch of work completes, review the catalog:
- Are there categories with many entries? That's a systemic gap.
- Are there OPEN entries? Those are unmitigated risks.
- Are the prevention mechanisms actually being followed?

## When to Use

- Start the catalog after your first coordination failure (you'll know it when it happens)
- Add entries in real-time — don't batch them
- Review periodically — the patterns that emerge are more valuable than individual entries

## Real Example

The proving-ground project accumulated 6 coordination failures over approximately two weeks of multi-agent construction. The failures clustered in two categories: Partial Update (3 incidents) and Context Starvation (2 incidents). This clustering revealed that the project's biggest gap was not dispatch quality or session management, but **cross-file consistency** — the problem of keeping multiple documents synchronized when a shared fact changes.

That insight led directly to the Cross-File Consistency Gate pattern, which eliminated the Partial Update category entirely in subsequent work.

## Anti-Patterns

- **Not recording failures.** "It worked out in the end" doesn't prevent recurrence. The fix might have taken an entire session that could have been avoided.
- **Blaming the agent.** The agent did what it was told with the context it had. The failure is in the protocol, not the execution.
- **Recording too much detail.** The catalog should be scannable. Root cause + prevention is more valuable than a blow-by-blow transcript.
- **Not reviewing the catalog.** A catalog nobody reads is just documentation debt. Review it when planning new work to check if known failure modes apply.
